{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mlpackage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix seed value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "def set_seed(manualSeed: int):\n",
    "    # Fix the seed value\n",
    "    random.seed(manualSeed)\n",
    "    torch.manual_seed(manualSeed)\n",
    "    torch.cuda.manual_seed_all(manualSeed)\n",
    "    # if True, causes cuDNN to benchmark multiple convolution algorithms and select the fastest.\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.deterministic = (\n",
    "        True  # if True, causes cuDNN to only use deterministic convolution algorithms.\n",
    "    )\n",
    "    torch.backends.cudnn.benchmark = False  # if True, causes cuDNN to benchmark multiple convolution algorithms and select the fastest.\n",
    "\n",
    "\n",
    "class WorkerInitializer:\n",
    "    def __init__(self, manualSeed: int):\n",
    "        self.manualSeed = manualSeed\n",
    "\n",
    "    def worker_init_fn(self, worker_id: int):\n",
    "        random.seed(self.manualSeed + worker_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "manualSeed = 42\n",
    "set_seed(manualSeed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create ResNet20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "\n",
    "\n",
    "# ResNet for CIFAR\n",
    "class DownsampleA(nn.Module):\n",
    "    def __init__(self, nIn, nOut, stride):\n",
    "        super(DownsampleA, self).__init__()\n",
    "        assert stride == 2\n",
    "        self.avg = nn.AvgPool2d(kernel_size=1, stride=stride)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.avg(x)\n",
    "        return torch.cat((x, x.mul(0)), 1)\n",
    "\n",
    "\n",
    "class DownsampleC(nn.Module):\n",
    "    def __init__(self, nIn, nOut, stride):\n",
    "        super(DownsampleC, self).__init__()\n",
    "        assert stride != 1 or nIn != nOut\n",
    "        self.conv = nn.Conv2d(\n",
    "            nIn, nOut, kernel_size=1, stride=stride, padding=0, bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DownsampleD(nn.Module):\n",
    "    def __init__(self, nIn, nOut, stride):\n",
    "        super(DownsampleD, self).__init__()\n",
    "        assert stride == 2\n",
    "        self.conv = nn.Conv2d(\n",
    "            nIn, nOut, kernel_size=2, stride=stride, padding=0, bias=False\n",
    "        )\n",
    "        self.bn = nn.BatchNorm2d(nOut)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResNetBasicblock(nn.Module):\n",
    "    expansion = 1\n",
    "    \"\"\"\n",
    "    RexNet basicblock (https://github.com/facebook/fb.resnet.torch/blob/master/models/resnet.lua)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(ResNetBasicblock, self).__init__()\n",
    "\n",
    "        self.conv_a = nn.Conv2d(\n",
    "            inplanes, planes, kernel_size=3, stride=stride, padding=1, bias=False\n",
    "        )\n",
    "        self.bn_a = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.conv_b = nn.Conv2d(\n",
    "            planes, planes, kernel_size=3, stride=1, padding=1, bias=False\n",
    "        )\n",
    "        self.bn_b = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        basicblock = self.conv_a(x)\n",
    "        basicblock = self.bn_a(basicblock)\n",
    "        basicblock = F.relu(basicblock, inplace=True)\n",
    "\n",
    "        basicblock = self.conv_b(basicblock)\n",
    "        basicblock = self.bn_b(basicblock)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        return F.relu(residual + basicblock, inplace=True)\n",
    "\n",
    "\n",
    "class CifarResNet(nn.Module):\n",
    "    \"\"\"\n",
    "    ResNet optimized for the Cifar dataset, as specified in\n",
    "    https://arxiv.org/abs/1512.03385.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, block, depth, num_classes):\n",
    "        \"\"\"Constructor\n",
    "        Args:\n",
    "            depth: number of layers.\n",
    "            num_classes: number of classes\n",
    "            base_width: base width\n",
    "        \"\"\"\n",
    "        super(CifarResNet, self).__init__()\n",
    "\n",
    "        # Model type specifies number of layers for CIFAR-10 and CIFAR-100 model\n",
    "        assert (depth - 2) % 6 == 0, \"depth should be one of 20, 32, 44, 56, 110\"\n",
    "        layer_blocks = (depth - 2) // 6\n",
    "        # print ('CifarResNet : Depth : {} , Layers for each block : {}'.format(depth, layer_blocks))\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "\n",
    "        self.inplanes = 16\n",
    "        self.layer1 = self._make_layer(block, 16, layer_blocks, stride=1)\n",
    "        self.layer2 = self._make_layer(block, 32, layer_blocks, stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, layer_blocks, stride=2)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        # self.avgpool = nn.AvgPool2d(8)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(64 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2.0 / n))\n",
    "                # m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                init.kaiming_normal_(m.weight)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = DownsampleA(self.inplanes, planes * block.expansion, stride)\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def resnet20(num_classes=10):\n",
    "    \"\"\"Constructs a ResNet-20 model for CIFAR-10 (by default)\n",
    "    Args:\n",
    "        num_classes (uint): number of classes\n",
    "    \"\"\"\n",
    "    model = CifarResNet(ResNetBasicblock, 20, num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = resnet20(num_classes=10).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/usr/local/lib/python3.10/dist-packages/torchvision/image.so: undefined symbol: _ZN3c107WarningC1ENS_7variantIJNS0_11UserWarningENS0_18DeprecationWarningEEEERKNS_14SourceLocationENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEb'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Prepare the CIFAR-100 for training\n",
    "batch_size = 128\n",
    "num_workers = 4\n",
    "\n",
    "\n",
    "cifar10_mean = (0.4914, 0.4822, 0.4465)\n",
    "cifar10_std = (0.2470, 0.2435, 0.2616)\n",
    "\n",
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(cifar10_mean, cifar10_std),\n",
    "    ]\n",
    ")\n",
    "test_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(cifar10_mean, cifar10_std),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_dataset = datasets.CIFAR10(\n",
    "    root=\"data\", train=True, download=True, transform=train_transform\n",
    ")\n",
    "test_dataset = datasets.CIFAR10(\n",
    "    root=\"data\", train=False, download=True, transform=test_transform\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    "    worker_init_fn=WorkerInitializer(manualSeed).worker_init_fn,\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    drop_last=False,\n",
    "    worker_init_fn=WorkerInitializer(manualSeed).worker_init_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epoch = 200\n",
    "\n",
    "optim_setting = {\n",
    "    \"name\": \"SGD\",\n",
    "    \"args\": {\n",
    "        \"lr\": 0.1,\n",
    "        \"momentum\": 0.9,\n",
    "        \"weight_decay\": 5e-4,\n",
    "        \"nesterov\": True,\n",
    "    },\n",
    "}\n",
    "scheduler_setting = {\n",
    "    \"name\": \"CosineAnnealingLR\",\n",
    "    \"args\": {\"T_max\": max_epoch, \"eta_min\": 0.0},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "def accuracy(\n",
    "    output: torch.Tensor, target: torch.Tensor, topk: tuple[int,] = (1,)\n",
    ") -> list[torch.Tensor]:\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(100 * correct_k / batch_size)\n",
    "    return res\n",
    "\n",
    "\n",
    "class AverageMeter:\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val: float = 0.0\n",
    "        self.avg: float = 0.0\n",
    "        self.sum: float = 0.0\n",
    "        self.count: int = 0\n",
    "\n",
    "    def update(self, val: float, n: int = 1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def save_checkpoint(model, save_dir, epoch, is_best=False):\n",
    "    state = {\n",
    "        \"epoch\": epoch,\n",
    "        \"arch\": model.__class__.__name__,\n",
    "        \"model_pram\": model.state_dict(),\n",
    "    }\n",
    "    if is_best:\n",
    "        path = os.path.join(save_dir, \"best_checkpoint.pkl\")\n",
    "    else:\n",
    "        path = os.path.join(save_dir, \"checkpoint_epoch_%d.pkl\" % epoch)\n",
    "    torch.save(state, path, pickle_protocol=4)\n",
    "\n",
    "\n",
    "def load_checkpoint(model, save_dir, epoch=1, is_best=False):\n",
    "    if is_best:\n",
    "        path = os.path.join(save_dir, \"best_checkpoint.pkl\")\n",
    "    else:\n",
    "        path = os.path.join(save_dir, \"checkpoint_epoch_%d.pkl\" % epoch)\n",
    "    state = torch.load(path, map_location=\"cpu\")\n",
    "    model.cpu()\n",
    "    model.load_state_dict(state[\"model_pram\"])\n",
    "    model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = getattr(torch.optim, optim_setting[\"name\"])(\n",
    "    base_model.parameters(), **optim_setting[\"args\"]\n",
    ")\n",
    "scheduler = getattr(torch.optim.lr_scheduler, scheduler_setting[\"name\"])(\n",
    "    optimizer, **scheduler_setting[\"args\"]\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "loss_meter=AverageMeter()\n",
    "top1_meter=AverageMeter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "loss :train=1.539   top1 :train=44.159\n",
      "top1 :test=51.720\n",
      "  elapsed_time:15.603[sec]\n",
      "epoch 2\n",
      "loss :train=1.035   top1 :train=62.997\n",
      "top1 :test=61.740\n",
      "  elapsed_time:14.840[sec]\n",
      "epoch 3\n",
      "loss :train=0.829   top1 :train=70.907\n",
      "top1 :test=69.200\n",
      "  elapsed_time:14.909[sec]\n",
      "epoch 4\n",
      "loss :train=0.726   top1 :train=74.752\n",
      "top1 :test=72.840\n",
      "  elapsed_time:15.197[sec]\n",
      "epoch 5\n",
      "loss :train=0.674   top1 :train=76.567\n",
      "top1 :test=71.860\n",
      "  elapsed_time:14.749[sec]\n",
      "epoch 6\n",
      "loss :train=0.631   top1 :train=78.265\n",
      "top1 :test=75.100\n",
      "  elapsed_time:14.577[sec]\n",
      "epoch 7\n",
      "loss :train=0.608   top1 :train=79.044\n",
      "top1 :test=72.030\n",
      "  elapsed_time:14.919[sec]\n",
      "epoch 8\n",
      "loss :train=0.588   top1 :train=79.665\n",
      "top1 :test=74.210\n",
      "  elapsed_time:14.665[sec]\n",
      "epoch 9\n",
      "loss :train=0.565   top1 :train=80.533\n",
      "top1 :test=71.540\n",
      "  elapsed_time:14.604[sec]\n",
      "epoch 10\n",
      "loss :train=0.557   top1 :train=80.731\n",
      "top1 :test=72.820\n",
      "  elapsed_time:14.750[sec]\n",
      "epoch 11\n",
      "loss :train=0.542   top1 :train=81.342\n",
      "top1 :test=71.130\n",
      "  elapsed_time:14.844[sec]\n",
      "epoch 12\n",
      "loss :train=0.530   top1 :train=81.715\n",
      "top1 :test=73.850\n",
      "  elapsed_time:14.535[sec]\n",
      "epoch 13\n",
      "loss :train=0.525   top1 :train=81.905\n",
      "top1 :test=74.640\n",
      "  elapsed_time:15.104[sec]\n",
      "epoch 14\n",
      "loss :train=0.520   top1 :train=82.167\n",
      "top1 :test=78.460\n",
      "  elapsed_time:15.228[sec]\n",
      "epoch 15\n",
      "loss :train=0.509   top1 :train=82.610\n",
      "top1 :test=79.210\n",
      "  elapsed_time:14.604[sec]\n",
      "epoch 16\n",
      "loss :train=0.500   top1 :train=82.885\n",
      "top1 :test=78.770\n",
      "  elapsed_time:15.060[sec]\n",
      "epoch 17\n",
      "loss :train=0.497   top1 :train=82.770\n",
      "top1 :test=77.180\n",
      "  elapsed_time:15.067[sec]\n",
      "epoch 18\n",
      "loss :train=0.493   top1 :train=83.033\n",
      "top1 :test=80.540\n",
      "  elapsed_time:14.886[sec]\n",
      "epoch 19\n",
      "loss :train=0.488   top1 :train=83.029\n",
      "top1 :test=74.300\n",
      "  elapsed_time:14.022[sec]\n",
      "epoch 20\n",
      "loss :train=0.481   top1 :train=83.317\n",
      "top1 :test=72.980\n",
      "  elapsed_time:14.271[sec]\n",
      "epoch 21\n",
      "loss :train=0.482   top1 :train=83.472\n",
      "top1 :test=78.590\n",
      "  elapsed_time:14.810[sec]\n",
      "epoch 22\n",
      "loss :train=0.475   top1 :train=83.658\n",
      "top1 :test=73.450\n",
      "  elapsed_time:15.009[sec]\n",
      "epoch 23\n",
      "loss :train=0.468   top1 :train=83.918\n",
      "top1 :test=78.500\n",
      "  elapsed_time:15.084[sec]\n",
      "epoch 24\n",
      "loss :train=0.468   top1 :train=83.996\n",
      "top1 :test=68.620\n",
      "  elapsed_time:15.545[sec]\n",
      "epoch 25\n",
      "loss :train=0.464   top1 :train=83.924\n",
      "top1 :test=69.350\n",
      "  elapsed_time:15.247[sec]\n",
      "epoch 26\n",
      "loss :train=0.459   top1 :train=84.111\n",
      "top1 :test=78.260\n",
      "  elapsed_time:14.558[sec]\n",
      "epoch 27\n",
      "loss :train=0.456   top1 :train=84.395\n",
      "top1 :test=78.490\n",
      "  elapsed_time:14.866[sec]\n",
      "epoch 28\n",
      "loss :train=0.455   top1 :train=84.303\n",
      "top1 :test=78.230\n",
      "  elapsed_time:15.104[sec]\n",
      "epoch 29\n",
      "loss :train=0.446   top1 :train=84.730\n",
      "top1 :test=59.570\n",
      "  elapsed_time:15.439[sec]\n",
      "epoch 30\n",
      "loss :train=0.449   top1 :train=84.481\n",
      "top1 :test=66.910\n",
      "  elapsed_time:14.748[sec]\n",
      "epoch 31\n",
      "loss :train=0.449   top1 :train=84.393\n",
      "top1 :test=80.150\n",
      "  elapsed_time:15.175[sec]\n",
      "epoch 32\n",
      "loss :train=0.442   top1 :train=84.874\n",
      "top1 :test=77.540\n",
      "  elapsed_time:14.910[sec]\n",
      "epoch 33\n",
      "loss :train=0.441   top1 :train=84.830\n",
      "top1 :test=81.810\n",
      "  elapsed_time:14.702[sec]\n",
      "epoch 34\n",
      "loss :train=0.439   top1 :train=84.918\n",
      "top1 :test=75.300\n",
      "  elapsed_time:14.275[sec]\n",
      "epoch 35\n",
      "loss :train=0.434   top1 :train=85.010\n",
      "top1 :test=81.480\n",
      "  elapsed_time:15.018[sec]\n",
      "epoch 36\n",
      "loss :train=0.435   top1 :train=85.080\n",
      "top1 :test=77.760\n",
      "  elapsed_time:15.004[sec]\n",
      "epoch 37\n",
      "loss :train=0.430   top1 :train=85.254\n",
      "top1 :test=81.340\n",
      "  elapsed_time:15.520[sec]\n",
      "epoch 38\n",
      "loss :train=0.430   top1 :train=85.066\n",
      "top1 :test=81.030\n",
      "  elapsed_time:14.921[sec]\n",
      "epoch 39\n",
      "loss :train=0.421   top1 :train=85.535\n",
      "top1 :test=78.450\n",
      "  elapsed_time:14.813[sec]\n",
      "epoch 40\n",
      "loss :train=0.423   top1 :train=85.447\n",
      "top1 :test=75.970\n",
      "  elapsed_time:15.141[sec]\n",
      "epoch 41\n",
      "loss :train=0.422   top1 :train=85.591\n",
      "top1 :test=81.980\n",
      "  elapsed_time:14.659[sec]\n",
      "epoch 42\n",
      "loss :train=0.417   top1 :train=85.563\n",
      "top1 :test=79.680\n",
      "  elapsed_time:14.425[sec]\n",
      "epoch 43\n",
      "loss :train=0.416   top1 :train=85.715\n",
      "top1 :test=77.910\n",
      "  elapsed_time:14.784[sec]\n",
      "epoch 44\n",
      "loss :train=0.415   top1 :train=85.759\n",
      "top1 :test=75.530\n",
      "  elapsed_time:14.824[sec]\n",
      "epoch 45\n",
      "loss :train=0.412   top1 :train=85.837\n",
      "top1 :test=79.130\n",
      "  elapsed_time:15.004[sec]\n",
      "epoch 46\n",
      "loss :train=0.406   top1 :train=85.948\n",
      "top1 :test=82.220\n",
      "  elapsed_time:15.010[sec]\n",
      "epoch 47\n",
      "loss :train=0.411   top1 :train=85.921\n",
      "top1 :test=73.600\n",
      "  elapsed_time:14.797[sec]\n",
      "epoch 48\n",
      "loss :train=0.410   top1 :train=85.980\n",
      "top1 :test=79.970\n",
      "  elapsed_time:15.290[sec]\n",
      "epoch 49\n",
      "loss :train=0.405   top1 :train=85.919\n",
      "top1 :test=73.920\n",
      "  elapsed_time:15.010[sec]\n",
      "epoch 50\n",
      "loss :train=0.400   top1 :train=86.248\n",
      "top1 :test=79.930\n",
      "  elapsed_time:13.876[sec]\n",
      "epoch 51\n",
      "loss :train=0.399   top1 :train=86.198\n",
      "top1 :test=81.050\n",
      "  elapsed_time:14.975[sec]\n",
      "epoch 52\n",
      "loss :train=0.400   top1 :train=86.296\n",
      "top1 :test=76.390\n",
      "  elapsed_time:14.848[sec]\n",
      "epoch 53\n",
      "loss :train=0.396   top1 :train=86.314\n",
      "top1 :test=80.840\n",
      "  elapsed_time:14.845[sec]\n",
      "epoch 54\n",
      "loss :train=0.397   top1 :train=86.288\n",
      "top1 :test=78.560\n",
      "  elapsed_time:14.986[sec]\n",
      "epoch 55\n",
      "loss :train=0.394   top1 :train=86.466\n",
      "top1 :test=82.060\n",
      "  elapsed_time:14.883[sec]\n",
      "epoch 56\n",
      "loss :train=0.396   top1 :train=86.476\n",
      "top1 :test=82.970\n",
      "  elapsed_time:14.745[sec]\n",
      "epoch 57\n",
      "loss :train=0.389   top1 :train=86.695\n",
      "top1 :test=82.310\n",
      "  elapsed_time:14.510[sec]\n",
      "epoch 58\n",
      "loss :train=0.384   top1 :train=86.923\n",
      "top1 :test=83.300\n",
      "  elapsed_time:14.844[sec]\n",
      "epoch 59\n",
      "loss :train=0.387   top1 :train=86.683\n",
      "top1 :test=81.580\n",
      "  elapsed_time:14.899[sec]\n",
      "epoch 60\n",
      "loss :train=0.384   top1 :train=86.729\n",
      "top1 :test=79.110\n",
      "  elapsed_time:14.909[sec]\n",
      "epoch 61\n",
      "loss :train=0.381   top1 :train=86.837\n",
      "top1 :test=75.710\n",
      "  elapsed_time:15.060[sec]\n",
      "epoch 62\n",
      "loss :train=0.380   top1 :train=86.965\n",
      "top1 :test=78.260\n",
      "  elapsed_time:14.899[sec]\n",
      "epoch 63\n",
      "loss :train=0.374   top1 :train=86.977\n",
      "top1 :test=82.560\n",
      "  elapsed_time:14.828[sec]\n",
      "epoch 64\n",
      "loss :train=0.375   top1 :train=87.149\n",
      "top1 :test=83.750\n",
      "  elapsed_time:14.096[sec]\n",
      "epoch 65\n",
      "loss :train=0.374   top1 :train=87.119\n",
      "top1 :test=81.190\n",
      "  elapsed_time:14.360[sec]\n",
      "epoch 66\n",
      "loss :train=0.374   top1 :train=87.011\n",
      "top1 :test=81.900\n",
      "  elapsed_time:14.647[sec]\n",
      "epoch 67\n",
      "loss :train=0.374   top1 :train=87.049\n",
      "top1 :test=83.970\n",
      "  elapsed_time:15.162[sec]\n",
      "epoch 68\n",
      "loss :train=0.369   top1 :train=87.316\n",
      "top1 :test=81.340\n",
      "  elapsed_time:15.035[sec]\n",
      "epoch 69\n",
      "loss :train=0.366   top1 :train=87.344\n",
      "top1 :test=83.030\n",
      "  elapsed_time:14.433[sec]\n",
      "epoch 70\n",
      "loss :train=0.367   top1 :train=87.240\n",
      "top1 :test=82.890\n",
      "  elapsed_time:14.625[sec]\n",
      "epoch 71\n",
      "loss :train=0.364   top1 :train=87.340\n",
      "top1 :test=80.700\n",
      "  elapsed_time:14.687[sec]\n",
      "epoch 72\n",
      "loss :train=0.361   top1 :train=87.488\n",
      "top1 :test=83.520\n",
      "  elapsed_time:14.195[sec]\n",
      "epoch 73\n",
      "loss :train=0.358   top1 :train=87.538\n",
      "top1 :test=76.560\n",
      "  elapsed_time:14.700[sec]\n",
      "epoch 74\n",
      "loss :train=0.361   top1 :train=87.562\n",
      "top1 :test=85.460\n",
      "  elapsed_time:14.921[sec]\n",
      "epoch 75\n",
      "loss :train=0.355   top1 :train=87.710\n",
      "top1 :test=83.700\n",
      "  elapsed_time:15.305[sec]\n",
      "epoch 76\n",
      "loss :train=0.354   top1 :train=87.790\n",
      "top1 :test=76.940\n",
      "  elapsed_time:14.778[sec]\n",
      "epoch 77\n",
      "loss :train=0.355   top1 :train=87.704\n",
      "top1 :test=84.390\n",
      "  elapsed_time:14.903[sec]\n",
      "epoch 78\n",
      "loss :train=0.349   top1 :train=87.921\n",
      "top1 :test=82.250\n",
      "  elapsed_time:15.003[sec]\n",
      "epoch 79\n",
      "loss :train=0.353   top1 :train=87.736\n",
      "top1 :test=83.320\n",
      "  elapsed_time:15.076[sec]\n",
      "epoch 80\n",
      "loss :train=0.343   top1 :train=88.153\n",
      "top1 :test=82.690\n",
      "  elapsed_time:14.612[sec]\n",
      "epoch 81\n",
      "loss :train=0.345   top1 :train=88.093\n",
      "top1 :test=85.370\n",
      "  elapsed_time:15.159[sec]\n",
      "epoch 82\n",
      "loss :train=0.345   top1 :train=88.109\n",
      "top1 :test=85.320\n",
      "  elapsed_time:14.681[sec]\n",
      "epoch 83\n",
      "loss :train=0.342   top1 :train=88.257\n",
      "top1 :test=82.730\n",
      "  elapsed_time:15.117[sec]\n",
      "epoch 84\n",
      "loss :train=0.339   top1 :train=88.425\n",
      "top1 :test=80.860\n",
      "  elapsed_time:15.101[sec]\n",
      "epoch 85\n",
      "loss :train=0.335   top1 :train=88.231\n",
      "top1 :test=79.120\n",
      "  elapsed_time:14.660[sec]\n",
      "epoch 86\n",
      "loss :train=0.339   top1 :train=88.081\n",
      "top1 :test=82.310\n",
      "  elapsed_time:15.094[sec]\n",
      "epoch 87\n",
      "loss :train=0.329   top1 :train=88.810\n",
      "top1 :test=82.550\n",
      "  elapsed_time:14.964[sec]\n",
      "epoch 88\n",
      "loss :train=0.326   top1 :train=88.714\n",
      "top1 :test=84.340\n",
      "  elapsed_time:15.042[sec]\n",
      "epoch 89\n",
      "loss :train=0.329   top1 :train=88.592\n",
      "top1 :test=85.070\n",
      "  elapsed_time:14.806[sec]\n",
      "epoch 90\n",
      "loss :train=0.325   top1 :train=88.836\n",
      "top1 :test=82.080\n",
      "  elapsed_time:14.479[sec]\n",
      "epoch 91\n",
      "loss :train=0.319   top1 :train=89.046\n",
      "top1 :test=85.300\n",
      "  elapsed_time:14.309[sec]\n",
      "epoch 92\n",
      "loss :train=0.315   top1 :train=89.211\n",
      "top1 :test=84.250\n",
      "  elapsed_time:14.811[sec]\n",
      "epoch 93\n",
      "loss :train=0.322   top1 :train=88.970\n",
      "top1 :test=85.190\n",
      "  elapsed_time:14.878[sec]\n",
      "epoch 94\n",
      "loss :train=0.322   top1 :train=88.926\n",
      "top1 :test=82.160\n",
      "  elapsed_time:14.675[sec]\n",
      "epoch 95\n",
      "loss :train=0.314   top1 :train=89.219\n",
      "top1 :test=81.960\n",
      "  elapsed_time:14.683[sec]\n",
      "epoch 96\n",
      "loss :train=0.310   top1 :train=89.335\n",
      "top1 :test=80.960\n",
      "  elapsed_time:15.201[sec]\n",
      "epoch 97\n",
      "loss :train=0.310   top1 :train=89.437\n",
      "top1 :test=85.140\n",
      "  elapsed_time:14.837[sec]\n",
      "epoch 98\n",
      "loss :train=0.312   top1 :train=89.273\n",
      "top1 :test=83.390\n",
      "  elapsed_time:14.852[sec]\n",
      "epoch 99\n",
      "loss :train=0.309   top1 :train=89.403\n",
      "top1 :test=85.090\n",
      "  elapsed_time:14.751[sec]\n",
      "epoch 100\n",
      "loss :train=0.307   top1 :train=89.265\n",
      "top1 :test=85.310\n",
      "  elapsed_time:14.728[sec]\n",
      "epoch 101\n",
      "loss :train=0.300   top1 :train=89.587\n",
      "top1 :test=83.830\n",
      "  elapsed_time:14.611[sec]\n",
      "epoch 102\n",
      "loss :train=0.301   top1 :train=89.497\n",
      "top1 :test=85.810\n",
      "  elapsed_time:15.235[sec]\n",
      "epoch 103\n",
      "loss :train=0.300   top1 :train=89.665\n",
      "top1 :test=83.190\n",
      "  elapsed_time:15.027[sec]\n",
      "epoch 104\n",
      "loss :train=0.296   top1 :train=89.900\n",
      "top1 :test=84.800\n",
      "  elapsed_time:14.105[sec]\n",
      "epoch 105\n",
      "loss :train=0.289   top1 :train=90.000\n",
      "top1 :test=84.830\n",
      "  elapsed_time:14.516[sec]\n",
      "epoch 106\n",
      "loss :train=0.295   top1 :train=89.802\n",
      "top1 :test=84.850\n",
      "  elapsed_time:14.472[sec]\n",
      "epoch 107\n",
      "loss :train=0.288   top1 :train=90.220\n",
      "top1 :test=86.340\n",
      "  elapsed_time:15.149[sec]\n",
      "epoch 108\n",
      "loss :train=0.284   top1 :train=90.274\n",
      "top1 :test=83.980\n",
      "  elapsed_time:14.352[sec]\n",
      "epoch 109\n",
      "loss :train=0.281   top1 :train=90.312\n",
      "top1 :test=86.020\n",
      "  elapsed_time:14.900[sec]\n",
      "epoch 110\n",
      "loss :train=0.278   top1 :train=90.415\n",
      "top1 :test=85.470\n",
      "  elapsed_time:15.010[sec]\n",
      "epoch 111\n",
      "loss :train=0.281   top1 :train=90.150\n",
      "top1 :test=84.640\n",
      "  elapsed_time:14.471[sec]\n",
      "epoch 112\n",
      "loss :train=0.273   top1 :train=90.635\n",
      "top1 :test=85.150\n",
      "  elapsed_time:14.730[sec]\n",
      "epoch 113\n",
      "loss :train=0.272   top1 :train=90.595\n",
      "top1 :test=86.720\n",
      "  elapsed_time:14.557[sec]\n",
      "epoch 114\n",
      "loss :train=0.270   top1 :train=90.479\n",
      "top1 :test=85.740\n",
      "  elapsed_time:14.871[sec]\n",
      "epoch 115\n",
      "loss :train=0.265   top1 :train=90.919\n",
      "top1 :test=86.530\n",
      "  elapsed_time:14.118[sec]\n",
      "epoch 116\n",
      "loss :train=0.267   top1 :train=90.709\n",
      "top1 :test=86.930\n",
      "  elapsed_time:14.764[sec]\n",
      "epoch 117\n",
      "loss :train=0.258   top1 :train=91.096\n",
      "top1 :test=85.240\n",
      "  elapsed_time:15.007[sec]\n",
      "epoch 118\n",
      "loss :train=0.261   top1 :train=90.958\n",
      "top1 :test=85.150\n",
      "  elapsed_time:14.842[sec]\n",
      "epoch 119\n",
      "loss :train=0.258   top1 :train=91.196\n",
      "top1 :test=85.260\n",
      "  elapsed_time:14.918[sec]\n",
      "epoch 120\n",
      "loss :train=0.253   top1 :train=91.420\n",
      "top1 :test=87.340\n",
      "  elapsed_time:15.169[sec]\n",
      "epoch 121\n",
      "loss :train=0.246   top1 :train=91.434\n",
      "top1 :test=87.620\n",
      "  elapsed_time:14.812[sec]\n",
      "epoch 122\n",
      "loss :train=0.246   top1 :train=91.496\n",
      "top1 :test=87.290\n",
      "  elapsed_time:14.617[sec]\n",
      "epoch 123\n",
      "loss :train=0.245   top1 :train=91.502\n",
      "top1 :test=86.950\n",
      "  elapsed_time:14.748[sec]\n",
      "epoch 124\n",
      "loss :train=0.239   top1 :train=91.765\n",
      "top1 :test=85.830\n",
      "  elapsed_time:14.108[sec]\n",
      "epoch 125\n",
      "loss :train=0.240   top1 :train=91.755\n",
      "top1 :test=87.440\n",
      "  elapsed_time:14.940[sec]\n",
      "epoch 126\n",
      "loss :train=0.236   top1 :train=91.791\n",
      "top1 :test=86.690\n",
      "  elapsed_time:15.070[sec]\n",
      "epoch 127\n",
      "loss :train=0.233   top1 :train=91.977\n",
      "top1 :test=88.090\n",
      "  elapsed_time:14.774[sec]\n",
      "epoch 128\n",
      "loss :train=0.229   top1 :train=92.089\n",
      "top1 :test=85.290\n",
      "  elapsed_time:15.020[sec]\n",
      "epoch 129\n",
      "loss :train=0.225   top1 :train=92.065\n",
      "top1 :test=88.490\n",
      "  elapsed_time:14.357[sec]\n",
      "epoch 130\n",
      "loss :train=0.217   top1 :train=92.632\n",
      "top1 :test=87.940\n",
      "  elapsed_time:14.520[sec]\n",
      "epoch 131\n",
      "loss :train=0.220   top1 :train=92.312\n",
      "top1 :test=88.360\n",
      "  elapsed_time:13.817[sec]\n",
      "epoch 132\n",
      "loss :train=0.215   top1 :train=92.512\n",
      "top1 :test=88.230\n",
      "  elapsed_time:15.049[sec]\n",
      "epoch 133\n",
      "loss :train=0.214   top1 :train=92.608\n",
      "top1 :test=88.350\n",
      "  elapsed_time:14.997[sec]\n",
      "epoch 134\n",
      "loss :train=0.213   top1 :train=92.652\n",
      "top1 :test=88.090\n",
      "  elapsed_time:15.159[sec]\n",
      "epoch 135\n",
      "loss :train=0.200   top1 :train=93.143\n",
      "top1 :test=86.800\n",
      "  elapsed_time:14.891[sec]\n",
      "epoch 136\n",
      "loss :train=0.204   top1 :train=93.017\n",
      "top1 :test=87.510\n",
      "  elapsed_time:15.084[sec]\n",
      "epoch 137\n",
      "loss :train=0.198   top1 :train=93.213\n",
      "top1 :test=86.850\n",
      "  elapsed_time:15.150[sec]\n",
      "epoch 138\n",
      "loss :train=0.195   top1 :train=93.211\n",
      "top1 :test=88.850\n",
      "  elapsed_time:14.795[sec]\n",
      "epoch 139\n",
      "loss :train=0.190   top1 :train=93.373\n",
      "top1 :test=88.470\n",
      "  elapsed_time:14.470[sec]\n",
      "epoch 140\n",
      "loss :train=0.189   top1 :train=93.427\n",
      "top1 :test=88.940\n",
      "  elapsed_time:14.767[sec]\n",
      "epoch 141\n",
      "loss :train=0.188   top1 :train=93.490\n",
      "top1 :test=89.390\n",
      "  elapsed_time:15.186[sec]\n",
      "epoch 142\n",
      "loss :train=0.183   top1 :train=93.664\n",
      "top1 :test=88.550\n",
      "  elapsed_time:15.111[sec]\n",
      "epoch 143\n",
      "loss :train=0.179   top1 :train=93.830\n",
      "top1 :test=88.920\n",
      "  elapsed_time:14.701[sec]\n",
      "epoch 144\n",
      "loss :train=0.182   top1 :train=93.682\n",
      "top1 :test=88.740\n",
      "  elapsed_time:14.721[sec]\n",
      "epoch 145\n",
      "loss :train=0.170   top1 :train=94.163\n",
      "top1 :test=89.340\n",
      "  elapsed_time:15.069[sec]\n",
      "epoch 146\n",
      "loss :train=0.164   top1 :train=94.325\n",
      "top1 :test=89.610\n",
      "  elapsed_time:14.906[sec]\n",
      "epoch 147\n",
      "loss :train=0.169   top1 :train=94.149\n",
      "top1 :test=89.080\n",
      "  elapsed_time:14.420[sec]\n",
      "epoch 148\n",
      "loss :train=0.158   top1 :train=94.555\n",
      "top1 :test=88.960\n",
      "  elapsed_time:14.883[sec]\n",
      "epoch 149\n",
      "loss :train=0.154   top1 :train=94.772\n",
      "top1 :test=90.090\n",
      "  elapsed_time:14.805[sec]\n",
      "epoch 150\n",
      "loss :train=0.148   top1 :train=94.872\n",
      "top1 :test=89.170\n",
      "  elapsed_time:15.125[sec]\n",
      "epoch 151\n",
      "loss :train=0.148   top1 :train=94.870\n",
      "top1 :test=89.520\n",
      "  elapsed_time:14.545[sec]\n",
      "epoch 152\n",
      "loss :train=0.142   top1 :train=95.062\n",
      "top1 :test=89.000\n",
      "  elapsed_time:14.477[sec]\n",
      "epoch 153\n",
      "loss :train=0.138   top1 :train=95.186\n",
      "top1 :test=90.300\n",
      "  elapsed_time:14.621[sec]\n",
      "epoch 154\n",
      "loss :train=0.135   top1 :train=95.363\n",
      "top1 :test=90.170\n",
      "  elapsed_time:14.513[sec]\n",
      "epoch 155\n",
      "loss :train=0.128   top1 :train=95.677\n",
      "top1 :test=89.830\n",
      "  elapsed_time:15.204[sec]\n",
      "epoch 156\n",
      "loss :train=0.125   top1 :train=95.747\n",
      "top1 :test=90.330\n",
      "  elapsed_time:14.894[sec]\n",
      "epoch 157\n",
      "loss :train=0.122   top1 :train=95.837\n",
      "top1 :test=89.820\n",
      "  elapsed_time:14.418[sec]\n",
      "epoch 158\n",
      "loss :train=0.116   top1 :train=96.038\n",
      "top1 :test=90.780\n",
      "  elapsed_time:14.644[sec]\n",
      "epoch 159\n",
      "loss :train=0.114   top1 :train=96.114\n",
      "top1 :test=89.840\n",
      "  elapsed_time:14.752[sec]\n",
      "epoch 160\n",
      "loss :train=0.107   top1 :train=96.378\n",
      "top1 :test=90.570\n",
      "  elapsed_time:15.182[sec]\n",
      "epoch 161\n",
      "loss :train=0.106   top1 :train=96.468\n",
      "top1 :test=90.950\n",
      "  elapsed_time:15.172[sec]\n",
      "epoch 162\n",
      "loss :train=0.100   top1 :train=96.621\n",
      "top1 :test=90.310\n",
      "  elapsed_time:14.640[sec]\n",
      "epoch 163\n",
      "loss :train=0.095   top1 :train=96.797\n",
      "top1 :test=90.920\n",
      "  elapsed_time:14.846[sec]\n",
      "epoch 164\n",
      "loss :train=0.091   top1 :train=96.941\n",
      "top1 :test=90.580\n",
      "  elapsed_time:15.055[sec]\n",
      "epoch 165\n",
      "loss :train=0.089   top1 :train=97.111\n",
      "top1 :test=91.840\n",
      "  elapsed_time:14.655[sec]\n",
      "epoch 166\n",
      "loss :train=0.083   top1 :train=97.254\n",
      "top1 :test=90.770\n",
      "  elapsed_time:14.459[sec]\n",
      "epoch 167\n",
      "loss :train=0.078   top1 :train=97.436\n",
      "top1 :test=91.580\n",
      "  elapsed_time:15.161[sec]\n",
      "epoch 168\n",
      "loss :train=0.076   top1 :train=97.584\n",
      "top1 :test=91.560\n",
      "  elapsed_time:15.207[sec]\n",
      "epoch 169\n",
      "loss :train=0.070   top1 :train=97.790\n",
      "top1 :test=90.830\n",
      "  elapsed_time:15.083[sec]\n",
      "epoch 170\n",
      "loss :train=0.064   top1 :train=97.981\n",
      "top1 :test=91.450\n",
      "  elapsed_time:15.117[sec]\n",
      "epoch 171\n",
      "loss :train=0.064   top1 :train=97.915\n",
      "top1 :test=91.690\n",
      "  elapsed_time:14.864[sec]\n",
      "epoch 172\n",
      "loss :train=0.060   top1 :train=98.111\n",
      "top1 :test=91.420\n",
      "  elapsed_time:15.252[sec]\n",
      "epoch 173\n",
      "loss :train=0.056   top1 :train=98.245\n",
      "top1 :test=92.290\n",
      "  elapsed_time:14.758[sec]\n",
      "epoch 174\n",
      "loss :train=0.052   top1 :train=98.470\n",
      "top1 :test=92.200\n",
      "  elapsed_time:14.414[sec]\n",
      "epoch 175\n",
      "loss :train=0.046   top1 :train=98.712\n",
      "top1 :test=91.690\n",
      "  elapsed_time:14.593[sec]\n",
      "epoch 176\n",
      "loss :train=0.046   top1 :train=98.628\n",
      "top1 :test=92.040\n",
      "  elapsed_time:15.283[sec]\n",
      "epoch 177\n",
      "loss :train=0.043   top1 :train=98.758\n",
      "top1 :test=92.230\n",
      "  elapsed_time:14.952[sec]\n",
      "epoch 178\n",
      "loss :train=0.041   top1 :train=98.760\n",
      "top1 :test=92.630\n",
      "  elapsed_time:14.674[sec]\n",
      "epoch 179\n",
      "loss :train=0.037   top1 :train=99.032\n",
      "top1 :test=92.500\n",
      "  elapsed_time:14.674[sec]\n",
      "epoch 180\n",
      "loss :train=0.034   top1 :train=99.107\n",
      "top1 :test=92.530\n",
      "  elapsed_time:15.115[sec]\n",
      "epoch 181\n",
      "loss :train=0.033   top1 :train=99.101\n",
      "top1 :test=92.560\n",
      "  elapsed_time:15.280[sec]\n",
      "epoch 182\n",
      "loss :train=0.031   top1 :train=99.203\n",
      "top1 :test=92.750\n",
      "  elapsed_time:15.126[sec]\n",
      "epoch 183\n",
      "loss :train=0.029   top1 :train=99.303\n",
      "top1 :test=92.580\n",
      "  elapsed_time:14.747[sec]\n",
      "epoch 184\n",
      "loss :train=0.027   top1 :train=99.383\n",
      "top1 :test=92.940\n",
      "  elapsed_time:14.698[sec]\n",
      "epoch 185\n",
      "loss :train=0.026   top1 :train=99.395\n",
      "top1 :test=92.880\n",
      "  elapsed_time:17.419[sec]\n",
      "epoch 186\n",
      "loss :train=0.025   top1 :train=99.451\n",
      "top1 :test=92.730\n",
      "  elapsed_time:14.580[sec]\n",
      "epoch 187\n",
      "loss :train=0.024   top1 :train=99.527\n",
      "top1 :test=92.880\n",
      "  elapsed_time:16.065[sec]\n",
      "epoch 188\n",
      "loss :train=0.023   top1 :train=99.489\n",
      "top1 :test=92.820\n",
      "  elapsed_time:14.701[sec]\n",
      "epoch 189\n",
      "loss :train=0.023   top1 :train=99.565\n",
      "top1 :test=92.790\n",
      "  elapsed_time:16.170[sec]\n",
      "epoch 190\n",
      "loss :train=0.022   top1 :train=99.575\n",
      "top1 :test=92.850\n",
      "  elapsed_time:15.313[sec]\n",
      "epoch 191\n",
      "loss :train=0.021   top1 :train=99.581\n",
      "top1 :test=92.890\n",
      "  elapsed_time:14.789[sec]\n",
      "epoch 192\n",
      "loss :train=0.020   top1 :train=99.617\n",
      "top1 :test=92.850\n",
      "  elapsed_time:14.892[sec]\n",
      "epoch 193\n",
      "loss :train=0.020   top1 :train=99.615\n",
      "top1 :test=92.890\n",
      "  elapsed_time:14.648[sec]\n",
      "epoch 194\n",
      "loss :train=0.019   top1 :train=99.675\n",
      "top1 :test=92.860\n",
      "  elapsed_time:14.738[sec]\n",
      "epoch 195\n",
      "loss :train=0.020   top1 :train=99.655\n",
      "top1 :test=92.980\n",
      "  elapsed_time:14.657[sec]\n",
      "epoch 196\n",
      "loss :train=0.020   top1 :train=99.617\n",
      "top1 :test=92.940\n",
      "  elapsed_time:14.459[sec]\n",
      "epoch 197\n",
      "loss :train=0.019   top1 :train=99.661\n",
      "top1 :test=92.860\n",
      "  elapsed_time:14.123[sec]\n",
      "epoch 198\n",
      "loss :train=0.020   top1 :train=99.649\n",
      "top1 :test=92.920\n",
      "  elapsed_time:15.048[sec]\n",
      "epoch 199\n",
      "loss :train=0.020   top1 :train=99.607\n",
      "top1 :test=92.890\n",
      "  elapsed_time:15.160[sec]\n",
      "epoch 200\n",
      "loss :train=0.019   top1 :train=99.661\n",
      "top1 :test=92.790\n",
      "  elapsed_time:14.254[sec]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "save_dir = \"checkpoint/resnet20\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "best_top1 = 0\n",
    "for epoch in range(1, max_epoch + 1):\n",
    "    print(f\"epoch {epoch}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    base_model.train()\n",
    "    for image, label in train_dataloader:\n",
    "        image = image.cuda()\n",
    "        label = label.cuda()\n",
    "        \n",
    "        base_model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        criterion.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            y = base_model(image)\n",
    "            loss = criterion(y, label)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        [top1] = accuracy(y, label, topk=(1,))\n",
    "        loss_meter.update(loss.item(), label.size(0))\n",
    "        top1_meter.update(top1.item(), label.size(0))\n",
    "    scheduler.step()\n",
    "    print(\n",
    "        \"loss :train={0:.3f}   top1 :train={1:.3f}\".format(\n",
    "            loss_meter.avg, top1_meter.avg\n",
    "        )\n",
    "    )\n",
    "    loss_meter.reset()\n",
    "    top1_meter.reset()\n",
    "\n",
    "    base_model.eval()\n",
    "    for image, label in test_dataloader:\n",
    "        image = image.cuda()\n",
    "        label = label.cuda()\n",
    "        \n",
    "        with torch.cuda.amp.autocast():\n",
    "            with torch.no_grad():\n",
    "                y = base_model(image)\n",
    "        [top1] = accuracy(y, label, topk=(1,))\n",
    "        top1_meter.update(top1.item(), label.size(0))\n",
    "    print(\n",
    "        \"top1 :test={0:.3f}\".format(\n",
    "            top1_meter.avg\n",
    "        )\n",
    "    )\n",
    "    if best_top1 <= top1_meter.avg:\n",
    "        save_checkpoint(base_model, save_dir, epoch, is_best=True)\n",
    "        best_top1 = top1_meter.avg\n",
    "    top1_meter.reset()\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(\"  elapsed_time:{0:.3f}[sec]\".format(elapsed_time))\n",
    "\n",
    "load_checkpoint(base_model, save_dir, is_best=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create mlpackage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchClassificationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TorchClassificationModel, self).__init__()\n",
    "        self.names = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
    "        self.layers = nn.Sequential(\n",
    "            base_model,\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "model = TorchClassificationModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(96.8750)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cpu()\n",
    "model.eval()\n",
    "image, label = next(iter(test_dataloader))\n",
    "output = model(image)\n",
    "accuracy(output, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_input = torch.rand(1, 3, 32, 32) \n",
    "traced_model = torch.jit.trace(model, example_input)\n",
    "out = traced_model(example_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "When both 'convert_to' and 'minimum_deployment_target' not specified, 'convert_to' is set to \"mlprogram\" and 'minimum_deployment_targer' is set to ct.target.iOS15 (which is same as ct.target.macOS12). Note: the model will not run on systems older than iOS15/macOS12/watchOS8/tvOS15. In order to make your model run on older system, please set the 'minimum_deployment_target' to iOS14/iOS13. Details please see the link: https://coremltools.readme.io/docs/unified-conversion-api#target-conversion-formats\n",
      "Converting PyTorch Frontend ==> MIL Ops:  99%|█████████▉| 173/174 [00:00<00:00, 1331.31 ops/s]\n",
      "Running MIL frontend_pytorch pipeline: 100%|██████████| 5/5 [00:00<00:00, 931.82 passes/s]\n",
      "Running MIL default pipeline: 100%|██████████| 71/71 [00:01<00:00, 64.26 passes/s] \n",
      "Running MIL backend_mlprogram pipeline: 100%|██████████| 12/12 [00:00<00:00, 329.95 passes/s]\n"
     ]
    }
   ],
   "source": [
    "import coremltools as ct\n",
    "\n",
    "scale = 1/((sum(cifar10_std)/3)*255.0)\n",
    "bias = [- cifar10_mean[0]/cifar10_std[0] , - cifar10_mean[1]/cifar10_std[1], - cifar10_mean[2]/cifar10_std[2]]\n",
    "\n",
    "classifier_config = ct.ClassifierConfig(model.names)\n",
    "image_input = ct.ImageType(name=\"image\",\n",
    "                           shape=example_input.shape,\n",
    "                           scale=scale, bias=bias)\n",
    "\n",
    "mlmodel = ct.convert(\n",
    "    traced_model,\n",
    "    classifier_config=classifier_config,\n",
    "    inputs=[image_input]\n",
    ")\n",
    "mlmodel.save(\"ResNet20CIFAR10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
